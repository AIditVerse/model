{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def tokenize_code(code, file_path):\n",
    "    tokens = []\n",
    "    reader = BytesIO(code.encode('utf-8')).readline\n",
    "    try:\n",
    "        for toknum, tokval, _, _, _ in tokenize.tokenize(reader):\n",
    "            if toknum != tokenize.ENCODING:\n",
    "                tokens.append(tokval)\n",
    "    except tokenize.TokenError as e:\n",
    "        print(\"Error tokenizing code in file:\", file_path)\n",
    "    return tokens\n",
    "\n",
    "def normalize_code(code):\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    return code\n",
    "\n",
    "def preprocess_code(code, file_path):\n",
    "    normalized_code = normalize_code(code)\n",
    "    tokens = tokenize_code(normalized_code, file_path)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load data from directories\n",
    "\n",
    "def load_data_from_directory(directory, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".sol\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                try:\n",
    "                    code = file.read()\n",
    "                    preprocessed_code = preprocess_code(code, filepath)\n",
    "                    data.append(preprocessed_code)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(\"Error processing file:\", filepath)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the directories\n",
    "vulnerable_dir = './Contracts for training/Re-entrancy'\n",
    "non_vulnerable_dir = './Contracts for training/Verified'\n",
    "\n",
    "# Load and label the data\n",
    "vulnerable_data, vulnerable_labels = load_data_from_directory(vulnerable_dir, 1)\n",
    "non_vulnerable_data, non_vulnerable_labels = load_data_from_directory(non_vulnerable_dir, 0)\n",
    "\n",
    "# Combine the data and labels\n",
    "data = vulnerable_data + non_vulnerable_data\n",
    "labels = vulnerable_labels + non_vulnerable_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_nn = vectorizer.fit_transform(data).toarray().astype('float32')  # Ensure dtype float32\n",
    "y_nn = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # Change dtype to float32 and reshape to [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted vectorizer\n",
    "joblib.dump(vectorizer, 'neural_network_vectors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data).toarray()\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_vectors.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fitted vectorizer\n",
    "joblib.dump(vectorizer, 'svm_vectors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class SmartContractVulnerabilityModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1=256, hidden_dim2=128):\n",
    "        super(SmartContractVulnerabilityModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skorch wrapper for the PyTorch model\n",
    "net = NeuralNetClassifier(\n",
    "    SmartContractVulnerabilityModel,\n",
    "    module__input_dim=1000,\n",
    "    max_epochs=20,  # Higher number of epochs with early stopping\n",
    "    lr=0.001,\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5)],  # Early stopping after 5 epochs without improvement\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "params = {\n",
    "    'lr': [0.001],\n",
    "    'max_epochs': [20],\n",
    "    'module__hidden_dim1': [256],\n",
    "    'module__hidden_dim2': [128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.1907\u001b[0m       \u001b[32m0.9263\u001b[0m        \u001b[35m0.1736\u001b[0m  57.7204\n",
      "      2        \u001b[36m0.1680\u001b[0m       \u001b[32m0.9302\u001b[0m        \u001b[35m0.1670\u001b[0m  37.0448\n",
      "      3        \u001b[36m0.1608\u001b[0m       \u001b[32m0.9311\u001b[0m        \u001b[35m0.1649\u001b[0m  37.3297\n",
      "      4        \u001b[36m0.1564\u001b[0m       \u001b[32m0.9341\u001b[0m        \u001b[35m0.1606\u001b[0m  36.0267\n",
      "      5        \u001b[36m0.1528\u001b[0m       0.9335        \u001b[35m0.1605\u001b[0m  35.1116\n",
      "      6        \u001b[36m0.1503\u001b[0m       0.9333        0.1609  37.3694\n",
      "      7        \u001b[36m0.1477\u001b[0m       0.9325        0.1612  35.4650\n",
      "      8        \u001b[36m0.1453\u001b[0m       0.9339        0.1621  37.9463\n",
      "      9        \u001b[36m0.1431\u001b[0m       \u001b[32m0.9347\u001b[0m        0.1621  35.3213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.1896\u001b[0m       \u001b[32m0.9260\u001b[0m        \u001b[35m0.1733\u001b[0m  35.0876\n",
      "      2        \u001b[36m0.1665\u001b[0m       \u001b[32m0.9309\u001b[0m        \u001b[35m0.1657\u001b[0m  35.1222\n",
      "      3        \u001b[36m0.1593\u001b[0m       \u001b[32m0.9310\u001b[0m        0.1665  37.5632\n",
      "      4        \u001b[36m0.1548\u001b[0m       \u001b[32m0.9331\u001b[0m        \u001b[35m0.1607\u001b[0m  35.0286\n",
      "      5        \u001b[36m0.1517\u001b[0m       0.9317        0.1639  36.7096\n",
      "      6        \u001b[36m0.1489\u001b[0m       \u001b[32m0.9352\u001b[0m        \u001b[35m0.1598\u001b[0m  35.4376\n",
      "      7        \u001b[36m0.1463\u001b[0m       0.9347        0.1620  37.0644\n",
      "      8        \u001b[36m0.1441\u001b[0m       \u001b[32m0.9357\u001b[0m        0.1611  36.8327\n",
      "      9        \u001b[36m0.1418\u001b[0m       0.9339        0.1640  42.8652\n",
      "     10        \u001b[36m0.1399\u001b[0m       0.9345        0.1646  40.8518\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.1907\u001b[0m       \u001b[32m0.9251\u001b[0m        \u001b[35m0.1753\u001b[0m  37.1350\n",
      "      2        \u001b[36m0.1675\u001b[0m       \u001b[32m0.9298\u001b[0m        \u001b[35m0.1673\u001b[0m  38.2859\n",
      "      3        \u001b[36m0.1602\u001b[0m       \u001b[32m0.9326\u001b[0m        \u001b[35m0.1625\u001b[0m  39.8847\n",
      "      4        \u001b[36m0.1557\u001b[0m       0.9321        0.1634  38.5396\n",
      "      5        \u001b[36m0.1522\u001b[0m       \u001b[32m0.9353\u001b[0m        \u001b[35m0.1585\u001b[0m  35.5824\n",
      "      6        \u001b[36m0.1493\u001b[0m       \u001b[32m0.9354\u001b[0m        0.1607  36.0319\n",
      "      7        \u001b[36m0.1469\u001b[0m       0.9342        0.1604  35.7726\n",
      "      8        \u001b[36m0.1443\u001b[0m       0.9332        0.1654  37.3244\n",
      "      9        \u001b[36m0.1421\u001b[0m       \u001b[32m0.9354\u001b[0m        0.1636  42.3677\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.1907\u001b[0m       \u001b[32m0.9268\u001b[0m        \u001b[35m0.1735\u001b[0m  39.5098\n",
      "      2        \u001b[36m0.1684\u001b[0m       \u001b[32m0.9277\u001b[0m        \u001b[35m0.1686\u001b[0m  39.5668\n",
      "      3        \u001b[36m0.1610\u001b[0m       \u001b[32m0.9320\u001b[0m        \u001b[35m0.1625\u001b[0m  35.3037\n",
      "      4        \u001b[36m0.1563\u001b[0m       \u001b[32m0.9330\u001b[0m        \u001b[35m0.1618\u001b[0m  37.0546\n",
      "      5        \u001b[36m0.1525\u001b[0m       0.9325        0.1640  46.6976\n",
      "      6        \u001b[36m0.1494\u001b[0m       \u001b[32m0.9345\u001b[0m        \u001b[35m0.1614\u001b[0m  40.6378\n",
      "      7        \u001b[36m0.1469\u001b[0m       \u001b[32m0.9350\u001b[0m        \u001b[35m0.1605\u001b[0m  40.4636\n",
      "      8        \u001b[36m0.1443\u001b[0m       0.9348        0.1615  73.8632\n",
      "      9        \u001b[36m0.1419\u001b[0m       0.9344        0.1632  123.7234\n"
     ]
    }
   ],
   "source": [
    "# Initialize GridSearchCV\n",
    "#cv is for cross validation\n",
    "#n_jobs=-1 means use all available cores\n",
    "gs = GridSearchCV(net, params, refit=True, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform grid search\n",
    "gs.fit(X_nn, y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'lr': 0.001, 'max_epochs': 20, 'module__hidden_dim1': 256, 'module__hidden_dim2': 128}\n",
      "Best score: 0.9341476835048672\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found:\", gs.best_params_)\n",
    "print(\"Best score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "torch.save(gs.best_estimator_.module_.state_dict(), 'neural_network_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_nn, y_nn, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Use the best model found by GridSearchCV\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mgs\u001b[49m\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fit the best model on the entire training set\u001b[39;00m\n\u001b[0;32m      8\u001b[0m best_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the best model found by GridSearchCV\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# Fit the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneural_network_vectors.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming you have your test data and labels in X_test and y_test respectively\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# X_test = ...\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# y_test = ...\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Calculate the confusion matrix\u001b[39;00m\n\u001b[0;32m     16\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model = joblib.load('neural_network_vectors.pkl')\n",
    "\n",
    "# Assuming you have your test data and labels in X_test and y_test respectively\n",
    "# X_test = ...\n",
    "# y_test = ...\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for SVM\n",
    "params = {\n",
    "    'C': [10], #0.1,1,10\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "gs = GridSearchCV(svm_model, params, refit=True, cv=3, scoring='accuracy')\n",
    "\n",
    "# Perform grid search\n",
    "gs.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best parameters found:\", gs.best_params_)\n",
    "print(\"Best score:\", gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "joblib.dump(gs.best_estimator_, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use the best model found by GridSearchCV\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# Fit the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
